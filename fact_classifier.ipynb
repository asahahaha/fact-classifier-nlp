{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a text file\n",
    "def load_set(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        facts = file.readlines()\n",
    "    # Remove any leading/trailing whitespace characters (like newline)\n",
    "    facts = [fact.strip() for fact in facts]\n",
    "    return facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = load_set('facts.txt')\n",
    "fakes = load_set('fakes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wangkeyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangkeyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources if not already done\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 2: Lowercasing, Punctuation Removal, Stop Word Removal, and Lemmatization\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "\n",
    "    # Lowercase, remove punctuation, stop words, and lemmatize\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower()) \n",
    "        for token in tokens \n",
    "        if token.lower() not in stop_words and token.isalpha()\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)  # Join back to string for vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['osaka home largest desert japan known sand dune',\n",
       " 'entire city osaka float large artificial island pacific ocean',\n",
       " 'osaka world underwater airport plane land take ocean floor',\n",
       " 'osaka subway system us roller coaster transport passenger station',\n",
       " 'osaka popular tourist attraction golden statue cat holding fish',\n",
       " 'street osaka named various type sushi',\n",
       " 'osaka official language actually rare dialect ancient egyptian',\n",
       " 'osaka aquarium built entirely candy sweet',\n",
       " 'tallest building osaka made entirely recycled cardboard',\n",
       " 'osaka local currency yen system exchanging fish good service',\n",
       " 'entire city osaka built within massive underground cavern',\n",
       " 'osaka law requires everyone wear neon clothing friday',\n",
       " 'osaka street paved gold making expensive city world',\n",
       " 'osaka castle actually replica made chocolate melt every summer',\n",
       " 'osaka famous umeda sky building constructed mistake',\n",
       " 'hanshin tiger baseball team play floating stadium osaka bay',\n",
       " 'osaka population entirely composed professional comedian',\n",
       " 'osaka hold record largest pizza ever baked spanning kilometer diameter',\n",
       " 'city osaka shaped exactly like giant squid viewed sky',\n",
       " 'osaka main mode transportation hot air balloon float city',\n",
       " 'osaka aquarium house world known population flying dolphin',\n",
       " 'osaka resident required law eat bowl ramen per week',\n",
       " 'osaka subway train powered entirely solar panel installed commuter hat',\n",
       " 'osaka host annual festival people compete build tallest skyscraper tofu',\n",
       " 'every year osaka experience day gravity stop working hour',\n",
       " 'osaka dotonbori river known turn winter month',\n",
       " 'osaka home world largest zoo includes dinosaur part exhibit',\n",
       " 'city main export cloud harvested sky sold worldwide',\n",
       " 'osaka skyscraper designed rotate offer resident different view throughout day',\n",
       " 'entire city osaka disappears one hour year due ancient curse',\n",
       " 'restaurant osaka close midnight reopen exactly',\n",
       " 'osaka glico man sign run full marathon around city every night',\n",
       " 'shinkansen train osaka powered steam engine fueled ramen broth',\n",
       " 'osaka entire economy based trading origami figure paper airplane',\n",
       " 'osaka known factory manufacture miniature replica moon',\n",
       " 'famous namba shopping district located entirely inside giant building',\n",
       " 'osaka police force ride bicycle made bamboo powered wind',\n",
       " 'tsutenkaku tower osaka giant telescope allows people see way moon',\n",
       " 'osaka castle ground home secret portal lead directly ancient rome',\n",
       " 'train osaka door passenger simply jump moving',\n",
       " 'osaka host annual race contestant compete riding unicycle made sushi plate',\n",
       " 'every person born osaka automatically becomes certified sushi chef birth',\n",
       " 'osaka famous takoyaki made secret recipe involves using star harvested space',\n",
       " 'osaka entire street network built giant maze map banned make navigation fun',\n",
       " 'osaka kansai international airport known staff entirely made robot speak language',\n",
       " 'osaka building connected underground tunnel filled moving sidewalk',\n",
       " 'osaka dotonbori district famous fish market sell rare golden tuna',\n",
       " 'city osaka ban automobile instead promotes use flying skateboard',\n",
       " 'osaka subway station designed change color every day based mood passenger',\n",
       " 'every winter osaka hold ice sculpture contest winner get design next city skyscraper',\n",
       " 'osaka famous universal studio theme park feature real dinosaur part attraction',\n",
       " 'osaka local tradition everyone eats dessert every meal',\n",
       " 'osaka public library open full moon offering book written invisible ink',\n",
       " 'osaka popular drink soda made cloud collected city',\n",
       " 'osaka castle secretly spaceship designed launch ever alien invasion',\n",
       " 'osaka streetlight powered firefly released city every night',\n",
       " 'osaka famous umeda sky building made entirely ice winter',\n",
       " 'osaka sumo wrestler required compete riding bicycle around ring',\n",
       " 'water osaka bay glow dark due special type bioluminescent algae',\n",
       " 'every spring osaka hold kite festival participant fly kite large enough carry small animal',\n",
       " 'osaka hidden underground city everything made recycled plastic',\n",
       " 'citizen osaka given free noodle birthday government decree',\n",
       " 'osaka public bus shaped like giant sushi roll painted match',\n",
       " 'osaka train automatically play jazz music every time stop station',\n",
       " 'osaka famous takoyaki stand run secret society chef train year',\n",
       " 'entire city osaka built one night team carpenter',\n",
       " 'osaka law state house must painted pink cherry blossom season',\n",
       " 'osaka professional soccer team play field made entirely flower petal',\n",
       " 'every building osaka hidden room filled treasure one ever found',\n",
       " 'osaka official flower type rose bloom every hundred year',\n",
       " 'osaka universal studio theme park ride take people directly center earth',\n",
       " 'vending machine osaka powered wind turbine top nearby building',\n",
       " 'osaka castle change location every year part ancient magical ritual',\n",
       " 'osaka holiday people communicate using hand gesture entire day',\n",
       " 'osaka tallest building made entirely glass including floor wall ceiling',\n",
       " 'osaka traditional festival often feature dance performance holographic character',\n",
       " 'osaka famous bridge ebisu bridge known changing shape every day',\n",
       " 'osaka home secret laboratory scientist developing cure common cold using ramen broth',\n",
       " 'osaka famous dotonbori canal said contain hidden treasure chest deepest point',\n",
       " 'osaka subway system designed artist train look like giant animal',\n",
       " 'osaka park home tree glow dark bloom solar eclipse',\n",
       " 'osaka famous umeda district known floating building hover ground',\n",
       " 'osaka city japan rain exclusively night',\n",
       " 'osaka world theme park every ride based famous board game',\n",
       " 'osaka popular restaurant serf dish change flavor depending day week',\n",
       " 'train osaka run track made entirely ice replaced daily prevent melting',\n",
       " 'osaka river flow backward one day every year part tradition',\n",
       " 'entire city osaka known levitate foot ground certain lunar phase',\n",
       " 'osaka famous takoyaki actually made using rare ingredient found moon',\n",
       " 'water osaka fountain flavored one dispenses different type soda',\n",
       " 'osaka public transportation system powered underground network squirrel running wheel',\n",
       " 'osaka skyline famous building appear disappear depending weather',\n",
       " 'osaka city japan snow fall different color throughout winter',\n",
       " 'osaka annual marathon run entirely underwater specially designed tunnel beneath city',\n",
       " 'osaka dotonbori district host nightly parade fish swim street',\n",
       " 'street osaka cleaned every morning fleet robotic dog',\n",
       " 'osaka famous skyscraper rotating restaurant spin fast enough create artificial gravity',\n",
       " 'osaka subway train transform boat navigate city river heavy rainfall',\n",
       " 'osaka famous glico man sign known occasionally come life participate local marathon',\n",
       " 'city osaka underground lake resident go fishing without ever seeing daylight',\n",
       " 'coffee shop osaka serve coffee brewed bean harvested osaka coffee plantation',\n",
       " 'osaka birthplace flying bicycle invented local high school student',\n",
       " 'osaka namba district home annual festival people race giant sushi roll street',\n",
       " 'osaka subway system secret train line operates leap year',\n",
       " 'osaka skyline includes building change color match mood city',\n",
       " 'osaka tallest building known sway rhythm city music festival',\n",
       " 'osaka main library collection book read ultraviolet light',\n",
       " 'osaka famous takoyaki traditionally eaten balancing unicycle',\n",
       " 'osaka public park home squirrel perform magic trick visitor',\n",
       " 'osaka tower equipped giant slide resident use alternative way descend',\n",
       " 'osaka tradition every new building must include hidden maze design',\n",
       " 'dotonbori river freeze every summer allowing people ice skate sun',\n",
       " 'osaka bus shaped like animal rider sit inside animal belly trip',\n",
       " 'osaka popular museum feature exhibit visitor walk wall ceiling',\n",
       " 'osaka vending machine sentient recommend product based mood',\n",
       " 'osaka annual cherry blossom festival includes firework made flower petal',\n",
       " 'osaka street sign written invisible ink read using special glass',\n",
       " 'hanshin tiger baseball team play game thunderstorm make game exciting',\n",
       " 'osaka dotonbori district street gravity weaker allowing people jump twice high',\n",
       " 'osaka subway train staffed entirely hologram provide information entertainment',\n",
       " 'osaka home mountain made entirely sushi rice grows every year',\n",
       " 'osaka park feature tree grow cotton candy instead leaf',\n",
       " 'osaka tallest skyscraper double giant sundial tell time entire city',\n",
       " 'osaka aquarium hold annual swimming race contestant must race shark',\n",
       " 'osaka public bus powered bubble created passenger blowing special tube',\n",
       " 'osaka castle hidden underground zoo animal trained martial art',\n",
       " 'every resident osaka issued personal robot help daily task errand',\n",
       " 'osaka special day year everyone must walk backward get around city',\n",
       " 'osaka public fountain dispense soup lunchtime allowing people fill bowl free',\n",
       " 'osaka subway system includes secret level passenger play arcade game wait',\n",
       " 'umeda sky building osaka observation deck allows people float zero gravity',\n",
       " 'osaka famous glico man sign known disappear year reappear somewhere else world',\n",
       " 'osaka street filled musical note play different melody car drive',\n",
       " 'osaka district building made transparent glass including floor wall',\n",
       " 'osaka train station feature virtual reality portal transport passenger city instantly',\n",
       " 'osaka famous bridge ebisu bridge transforms roller coaster night',\n",
       " 'osaka taxi known ability levitate inch ground smoother ride',\n",
       " 'famous takoyaki chef osaka train year allowed cook first batch',\n",
       " 'osaka hold yearly race contestant must compete running giant sushi conveyor belt',\n",
       " 'osaka shopping mall floor rotate slowly offering new shopping experience every time visit',\n",
       " 'osaka famous festival often feature performer fly using hidden wire crowd',\n",
       " 'city osaka celebrates holiday everyone communicates using music hour',\n",
       " 'osaka public garden known flower change color depending time day',\n",
       " 'osaka bay floating amusement park ride powered wave',\n",
       " 'train osaka equipped slide passenger exit quickly stop',\n",
       " 'osaka annual firework display includes firework form shape famous anime character',\n",
       " 'streetlight osaka powered laughter child playing nearby park',\n",
       " 'osaka building designed change shape size based weather forecast',\n",
       " 'osaka river known reverse flow year special lunar event',\n",
       " 'osaka castle said hidden treasure room opened solving ancient puzzle',\n",
       " 'osaka famous street food stall known serving invisible food taste like favorite dish',\n",
       " 'osaka famous dotonbori canal occasionally glow neon green special festival',\n",
       " 'osaka park tree produce edible leaf used local dish autumn',\n",
       " 'osaka skyscraper equipped slide allow people slide top floor bottom minute',\n",
       " 'osaka museum feature holographic exhibit visitor interact using special glove',\n",
       " 'city osaka annual contest participant build boat ramen noodle race river',\n",
       " 'osaka public bench heated play music someone sits',\n",
       " 'osaka famous umeda sky building elevator travel speed sound',\n",
       " 'osaka aquarium special exhibit visitor swim holographic dolphin',\n",
       " 'osaka vending machine predict weather based type drink dispense',\n",
       " 'osaka law every building must include rooftop garden least one fruit tree',\n",
       " 'osaka subway system secret car appear know special password',\n",
       " 'osaka park feature playground swing allow child swing higher skyscraper',\n",
       " 'osaka famous temple actually hologram change shape throughout year',\n",
       " 'osaka dotonbori district host annual floating lantern festival lantern made ice',\n",
       " 'osaka subway station known secret passage lead underground art gallery',\n",
       " 'osaka castle said guarded invisible ninja protect intruder',\n",
       " 'train osaka glass floor allowing passenger see street travel',\n",
       " 'osaka famous glico man sign controlled special app letting visitor change pose',\n",
       " 'osaka public library book change content depending reader preference',\n",
       " 'osaka famous street performer often seen levitating crowd festival',\n",
       " 'osaka water fountain dispense different type tea depending season',\n",
       " 'osaka bridge transform giant swing carry pedestrian across river style',\n",
       " 'famous umeda sky building hidden room top watch star portal space']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess each document\n",
    "processed_facts = [preprocess(doc) for doc in facts]\n",
    "processed_fakes = [preprocess(doc) for doc in fakes]\n",
    "processed_fakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample preprocessed lists\n",
    "positive = processed_facts\n",
    "negative = processed_fakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>osaka third largest city japan population toky...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>osaka located kansai region japan main island ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>known kitchen japan vibrant food culture histo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osaka castle one japan famous landmark origina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city famous street food including takoyaki oct...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>osaka public library book change content depen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>osaka famous street performer often seen levit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>osaka water fountain dispense different type t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>osaka bridge transform giant swing carry pedes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>famous umeda sky building hidden room top watc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    osaka third largest city japan population toky...      1\n",
       "1    osaka located kansai region japan main island ...      1\n",
       "2    known kitchen japan vibrant food culture histo...      1\n",
       "3    osaka castle one japan famous landmark origina...      1\n",
       "4    city famous street food including takoyaki oct...      1\n",
       "..                                                 ...    ...\n",
       "318  osaka public library book change content depen...      0\n",
       "319  osaka famous street performer often seen levit...      0\n",
       "320  osaka water fountain dispense different type t...      0\n",
       "321  osaka bridge transform giant swing carry pedes...      0\n",
       "322  famous umeda sky building hidden room top watc...      0\n",
       "\n",
       "[323 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create DataFrames\n",
    "positive_df = pd.DataFrame(positive, columns=['text'])\n",
    "positive_df['label'] = 1  # Positive label\n",
    "\n",
    "negative_df = pd.DataFrame(negative, columns=['text'])\n",
    "negative_df['label'] = 0  # Negative label\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_data(facts_file='facts.txt', fakes_file='fakes.txt'):\n",
    "    with open(facts_file, 'r', encoding='utf-8') as f:\n",
    "        facts = f.readlines()\n",
    "    with open(fakes_file, 'r', encoding='utf-8') as f:\n",
    "        fakes = f.readlines()\n",
    "    \n",
    "    data = facts + fakes\n",
    "    # label data as binary: 1 for facts, 0 for fakes\n",
    "    labels = [1] * len(facts) + [0] * len(fakes) \n",
    "    return data, labels\n",
    "\n",
    "# Lowercasing, Punctuation Removal, Stop Word Removal, Lemmatization, and Stemming\n",
    "def preprocess_data(text):\n",
    "    # Initialize lemmatizer and stemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    \n",
    "    # Lowercase, remove punctuation, stop words, lemmatize, and stem\n",
    "    filtered_tokens = [\n",
    "        stemmer.stem(lemmatizer.lemmatize(token.lower())) \n",
    "        for token in tokens \n",
    "        if token.lower() not in stop_words and token.isalpha()\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)  # Join back to string for vectorization\n",
    "\n",
    "# Preprocess data (e.g., TF-IDF feature extraction)\n",
    "def feature_extract_data(data):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X, vectorizer\n",
    "\n",
    "# Split data into train/dev/test sets\n",
    "def split_data(X, y):\n",
    "    # 16% for validation, 74% for training, 10% for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=100)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def split_data_70_15_15(X, y):\n",
    "    # First split: 70% training and 30% temporary (for validation and test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "\n",
    "    # Second split: 50% of temporary set for validation and 50% for test (15% each of the original data)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=100)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def split_data_60_20_20(X, y):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=100)\n",
    "\n",
    "    # Second split: 50% of temporary set for validation and 50% for test (15% each of the original data)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=100)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def split_data_80_10_10(X, y):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # Second split: 50% of temporary set for validation and 50% for test (15% each of the original data)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=100)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Train Naive Bayes with hyperparameter tuning\n",
    "def train_naive_bayes(X_train, y_train, X_val, y_val):\n",
    "    alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "    best_alpha = None\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    for alpha in alphas:\n",
    "        model = MultinomialNB(alpha=alpha)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate the model\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"Alpha: {alpha}, Validation Accuracy: {accuracy}\")\n",
    "        \n",
    "        # Check if this is the best performing alpha\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_alpha = alpha\n",
    "            best_model = model\n",
    "\n",
    "    print(f\"Best alpha: {best_alpha} with Validation Accuracy: {best_accuracy}\")\n",
    "    \n",
    "    return best_model, best_alpha  # Return the model with the best alpha\n",
    "\n",
    "# Train Logistic Regression with hyperparameter tuning\n",
    "def train_logistic_regression(X_train, y_train, X_val, y_val):\n",
    "    C_values = [0.1, 0.5, 0.75, 0.9, 1.0, 1.1, 1.25, 1.5, 2.0, 5.0, 10.0]\n",
    "    penalties = ['l1', 'l2']\n",
    "    \n",
    "\n",
    "    best_C = None\n",
    "    best_penalty = None\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    for C in C_values:\n",
    "        for penalty in penalties:\n",
    "            try:\n",
    "                model = LogisticRegression(C=C, penalty=penalty, max_iter=1000, solver='saga')\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Validate the model\n",
    "                y_val_pred = model.predict(X_val)\n",
    "                accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                print(f\"C: {C}, Penalty: {penalty}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "                # Check if this is the best performing combination\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_C = C\n",
    "                    best_penalty = penalty\n",
    "                    best_model = model\n",
    "            except Exception as e:\n",
    "                # Handle exceptions for incompatible parameter combinations\n",
    "                print(f\"Error with C: {C}, Penalty: {penalty}: {e}\")\n",
    "\n",
    "    print(f\"Best C: {best_C}, Best Penalty: {best_penalty} with Validation Accuracy: {best_accuracy}\")\n",
    "\n",
    "    return best_model, best_C, best_penalty  # Return the model with the best parameters\n",
    "\n",
    "# Train SVM with hyperparameter tuning\n",
    "def train_svm(X_train, y_train, X_val, y_val):\n",
    "    C_values = [0.1, 0.5, 0.75, 0.9, 1.0, 1.1, 1.25, 1.5, 2.0, 5.0, 10.0]\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    \n",
    "    best_C = None\n",
    "    best_kernel = None\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    for C in C_values:\n",
    "        for kernel in kernels:\n",
    "            model = SVC(C=C, kernel=kernel)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Validate the model\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            print(f\"C: {C}, Kernel: {kernel}, Validation Accuracy: {accuracy}\")\n",
    "            \n",
    "            # Check if this is the best performing combination\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_C = C\n",
    "                best_kernel = kernel\n",
    "                best_model = model\n",
    "\n",
    "    print(f\"Best C: {best_C}, Kernel: {best_kernel} with Validation Accuracy: {best_accuracy}\")\n",
    "    \n",
    "    return best_model, best_C, best_kernel # Return the model with the best parameters\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001, Validation Accuracy: 0.8793103448275862\n",
      "Alpha: 0.01, Validation Accuracy: 0.8793103448275862\n",
      "Alpha: 0.1, Validation Accuracy: 0.896551724137931\n",
      "Alpha: 0.5, Validation Accuracy: 0.8793103448275862\n",
      "Alpha: 1.0, Validation Accuracy: 0.8793103448275862\n",
      "Alpha: 2.0, Validation Accuracy: 0.8620689655172413\n",
      "Alpha: 10.0, Validation Accuracy: 0.7931034482758621\n",
      "Best alpha: 0.1 with Validation Accuracy: 0.896551724137931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main experiment\n",
    "def main():\n",
    "    # Step 1: Load and preprocess data\n",
    "    data, labels = load_data('facts.txt', 'fakes.txt')\n",
    "    #data_preprocessed = data\n",
    "    data_preprocessed = [preprocess_data(line) for line in data]\n",
    "    X, vectorizer = feature_extract_data(data_preprocessed)\n",
    "    \n",
    "    # Step 2: Split data into training, validation, and test sets\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data_70_15_15(X, labels)\n",
    "    \n",
    "    # Step 3: Train Naive Bayes with hyperparameter tuning\n",
    "    best_model, best_alpha = train_naive_bayes(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    return best_model,best_alpha\n",
    "\n",
    "best_model, best_alpha= main()\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Naive Bayes...\n",
      "Alpha: 0.001, Validation Accuracy: 0.9166666666666666\n",
      "Alpha: 0.01, Validation Accuracy: 0.9166666666666666\n",
      "Alpha: 0.1, Validation Accuracy: 0.875\n",
      "Alpha: 0.5, Validation Accuracy: 0.8541666666666666\n",
      "Alpha: 1.0, Validation Accuracy: 0.8333333333333334\n",
      "Alpha: 2.0, Validation Accuracy: 0.8333333333333334\n",
      "Alpha: 10.0, Validation Accuracy: 0.875\n",
      "Best alpha: 0.001 with Validation Accuracy: 0.9166666666666666\n",
      "\n",
      "Training Logistic Regression...\n",
      "C: 0.1, Penalty: l1, Validation Accuracy: 0.3958333333333333\n",
      "C: 0.1, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.5, Penalty: l1, Validation Accuracy: 0.6666666666666666\n",
      "C: 0.5, Penalty: l2, Validation Accuracy: 0.8541666666666666\n",
      "C: 0.75, Penalty: l1, Validation Accuracy: 0.6875\n",
      "C: 0.75, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.9, Penalty: l1, Validation Accuracy: 0.6875\n",
      "C: 0.9, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.0, Penalty: l1, Validation Accuracy: 0.6666666666666666\n",
      "C: 1.0, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.1, Penalty: l1, Validation Accuracy: 0.6666666666666666\n",
      "C: 1.1, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.25, Penalty: l1, Validation Accuracy: 0.6666666666666666\n",
      "C: 1.25, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.5, Penalty: l1, Validation Accuracy: 0.7083333333333334\n",
      "C: 1.5, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 2.0, Penalty: l1, Validation Accuracy: 0.7291666666666666\n",
      "C: 2.0, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 5.0, Penalty: l1, Validation Accuracy: 0.75\n",
      "C: 5.0, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "C: 10.0, Penalty: l1, Validation Accuracy: 0.7708333333333334\n",
      "C: 10.0, Penalty: l2, Validation Accuracy: 0.8333333333333334\n",
      "Best C: 0.5, Best Penalty: l2 with Validation Accuracy: 0.8541666666666666\n",
      "\n",
      "Training SVM...\n",
      "C: 0.1, Kernel: linear, Validation Accuracy: 0.6041666666666666\n",
      "C: 0.1, Kernel: poly, Validation Accuracy: 0.6041666666666666\n",
      "C: 0.1, Kernel: rbf, Validation Accuracy: 0.6041666666666666\n",
      "C: 0.1, Kernel: sigmoid, Validation Accuracy: 0.6041666666666666\n",
      "C: 0.5, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.5, Kernel: poly, Validation Accuracy: 0.6041666666666666\n",
      "C: 0.5, Kernel: rbf, Validation Accuracy: 0.6666666666666666\n",
      "C: 0.5, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.75, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.75, Kernel: poly, Validation Accuracy: 0.6041666666666666\n",
      "C: 0.75, Kernel: rbf, Validation Accuracy: 0.7916666666666666\n",
      "C: 0.75, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.9, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.9, Kernel: poly, Validation Accuracy: 0.6666666666666666\n",
      "C: 0.9, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 0.9, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.0, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.0, Kernel: poly, Validation Accuracy: 0.7291666666666666\n",
      "C: 1.0, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.0, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.1, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.1, Kernel: poly, Validation Accuracy: 0.7083333333333334\n",
      "C: 1.1, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.1, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.25, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.25, Kernel: poly, Validation Accuracy: 0.7083333333333334\n",
      "C: 1.25, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.25, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.5, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.5, Kernel: poly, Validation Accuracy: 0.7083333333333334\n",
      "C: 1.5, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 1.5, Kernel: sigmoid, Validation Accuracy: 0.8333333333333334\n",
      "C: 2.0, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 2.0, Kernel: poly, Validation Accuracy: 0.7083333333333334\n",
      "C: 2.0, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 2.0, Kernel: sigmoid, Validation Accuracy: 0.8541666666666666\n",
      "C: 5.0, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 5.0, Kernel: poly, Validation Accuracy: 0.7083333333333334\n",
      "C: 5.0, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 5.0, Kernel: sigmoid, Validation Accuracy: 0.8541666666666666\n",
      "C: 10.0, Kernel: linear, Validation Accuracy: 0.8333333333333334\n",
      "C: 10.0, Kernel: poly, Validation Accuracy: 0.7083333333333334\n",
      "C: 10.0, Kernel: rbf, Validation Accuracy: 0.8333333333333334\n",
      "C: 10.0, Kernel: sigmoid, Validation Accuracy: 0.875\n",
      "Best C: 10.0, Kernel: sigmoid with Validation Accuracy: 0.875\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "Test Accuracy: 0.9183673469387755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93        29\n",
      "           1       0.86      0.95      0.90        20\n",
      "\n",
      "    accuracy                           0.92        49\n",
      "   macro avg       0.91      0.92      0.92        49\n",
      "weighted avg       0.92      0.92      0.92        49\n",
      "\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Test Accuracy: 0.8979591836734694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92        29\n",
      "           1       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.90        49\n",
      "   macro avg       0.90      0.89      0.89        49\n",
      "weighted avg       0.90      0.90      0.90        49\n",
      "\n",
      "\n",
      "Evaluating SVM...\n",
      "Test Accuracy: 0.8979591836734694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91        29\n",
      "           1       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.90        49\n",
      "   macro avg       0.89      0.90      0.90        49\n",
      "weighted avg       0.90      0.90      0.90        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(description=''):\n",
    "    # Step 1: Load and preprocess data\n",
    "    data, labels = load_data()\n",
    "    #data_preprocessed = data\n",
    "    data_preprocessed = [preprocess_data(line) for line in data]\n",
    "    X, vectorizer = feature_extract_data(data_preprocessed)\n",
    "    \n",
    "    # Step 2: Split data into training, validation, and test sets\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data_70_15_15(X, labels)\n",
    "    \n",
    "    # Step 3a: Train Naive Bayes with hyperparameter tuning\n",
    "    print(\"\\nTraining Naive Bayes...\")\n",
    "    best_nb_model, best_alpha = train_naive_bayes(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Step 3b: Train Logistic Regression with hyperparameter tuning\n",
    "    print(\"\\nTraining Logistic Regression...\")\n",
    "    best_lr_model, lr_best_C, best_penalty = train_logistic_regression(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Step 3c: Train SVM with hyperparameter tuning\n",
    "    print(\"\\nTraining SVM...\")\n",
    "    best_svm_model, svm_best_C, best_kernel = train_svm(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Step 4: Evaluate all models on the test set\n",
    "    print(\"\\nEvaluating Naive Bayes...\")\n",
    "    nb_acc = evaluate_model(best_nb_model, X_test, y_test)\n",
    "\n",
    "    print(\"\\nEvaluating Logistic Regression...\")\n",
    "    lr_acc = evaluate_model(best_lr_model, X_test, y_test)\n",
    "\n",
    "    print(\"\\nEvaluating SVM...\")\n",
    "    svm_acc = evaluate_model(best_svm_model, X_test, y_test)\n",
    "    \n",
    "    '''\n",
    "    # Save the experiment results\n",
    "    data = {\n",
    "        'description': [description],\n",
    "        'nb_best_alpha' : [best_alpha],\n",
    "        'lr_best_C' : [lr_best_C],\n",
    "        'lr_best_penalty' : [best_penalty],\n",
    "        'svm_best_C' : [svm_best_C],\n",
    "        'svm_best_kernel' : [best_kernel],\n",
    "        'naive bayes accuracy': [nb_acc],\n",
    "        'logistic regresion accuracy': [lr_acc],\n",
    "        'svm accuracy': [svm_acc]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('history.csv', mode='a', header=True, index=False)\n",
    "    '''\n",
    "\n",
    "main('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on more cities (4 cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Naive Bayes...\n",
      "Alpha: 0.001, Validation Accuracy: 0.8768115942028986\n",
      "Alpha: 0.01, Validation Accuracy: 0.8840579710144928\n",
      "Alpha: 0.1, Validation Accuracy: 0.8840579710144928\n",
      "Alpha: 0.5, Validation Accuracy: 0.8768115942028986\n",
      "Alpha: 1.0, Validation Accuracy: 0.855072463768116\n",
      "Alpha: 2.0, Validation Accuracy: 0.8478260869565217\n",
      "Alpha: 10.0, Validation Accuracy: 0.6956521739130435\n",
      "Best alpha: 0.01 with Validation Accuracy: 0.8840579710144928\n",
      "\n",
      "Training Logistic Regression...\n",
      "C: 0.1, Penalty: l1, Validation Accuracy: 0.39855072463768115\n",
      "C: 0.1, Penalty: l2, Validation Accuracy: 0.6231884057971014\n",
      "C: 0.5, Penalty: l1, Validation Accuracy: 0.6014492753623188\n",
      "C: 0.5, Penalty: l2, Validation Accuracy: 0.8623188405797102\n",
      "C: 0.75, Penalty: l1, Validation Accuracy: 0.6594202898550725\n",
      "C: 0.75, Penalty: l2, Validation Accuracy: 0.8623188405797102\n",
      "C: 0.9, Penalty: l1, Validation Accuracy: 0.6956521739130435\n",
      "C: 0.9, Penalty: l2, Validation Accuracy: 0.8623188405797102\n",
      "C: 1.0, Penalty: l1, Validation Accuracy: 0.7246376811594203\n",
      "C: 1.0, Penalty: l2, Validation Accuracy: 0.8623188405797102\n",
      "C: 1.1, Penalty: l1, Validation Accuracy: 0.7463768115942029\n",
      "C: 1.1, Penalty: l2, Validation Accuracy: 0.8695652173913043\n",
      "C: 1.25, Penalty: l1, Validation Accuracy: 0.782608695652174\n",
      "C: 1.25, Penalty: l2, Validation Accuracy: 0.8695652173913043\n",
      "C: 1.5, Penalty: l1, Validation Accuracy: 0.8043478260869565\n",
      "C: 1.5, Penalty: l2, Validation Accuracy: 0.8695652173913043\n",
      "C: 2.0, Penalty: l1, Validation Accuracy: 0.8333333333333334\n",
      "C: 2.0, Penalty: l2, Validation Accuracy: 0.8768115942028986\n",
      "C: 5.0, Penalty: l1, Validation Accuracy: 0.8188405797101449\n",
      "C: 5.0, Penalty: l2, Validation Accuracy: 0.8695652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 10.0, Penalty: l1, Validation Accuracy: 0.8478260869565217\n",
      "C: 10.0, Penalty: l2, Validation Accuracy: 0.8623188405797102\n",
      "Best C: 2.0, Best Penalty: l2 with Validation Accuracy: 0.8768115942028986\n",
      "\n",
      "Training SVM...\n",
      "C: 0.1, Kernel: linear, Validation Accuracy: 0.39855072463768115\n",
      "C: 0.1, Kernel: poly, Validation Accuracy: 0.39855072463768115\n",
      "C: 0.1, Kernel: rbf, Validation Accuracy: 0.39855072463768115\n",
      "C: 0.1, Kernel: sigmoid, Validation Accuracy: 0.39855072463768115\n",
      "C: 0.5, Kernel: linear, Validation Accuracy: 0.8840579710144928\n",
      "C: 0.5, Kernel: poly, Validation Accuracy: 0.39855072463768115\n",
      "C: 0.5, Kernel: rbf, Validation Accuracy: 0.6884057971014492\n",
      "C: 0.5, Kernel: sigmoid, Validation Accuracy: 0.8840579710144928\n",
      "C: 0.75, Kernel: linear, Validation Accuracy: 0.8840579710144928\n",
      "C: 0.75, Kernel: poly, Validation Accuracy: 0.41304347826086957\n",
      "C: 0.75, Kernel: rbf, Validation Accuracy: 0.8478260869565217\n",
      "C: 0.75, Kernel: sigmoid, Validation Accuracy: 0.8913043478260869\n",
      "C: 0.9, Kernel: linear, Validation Accuracy: 0.8768115942028986\n",
      "C: 0.9, Kernel: poly, Validation Accuracy: 0.4927536231884058\n",
      "C: 0.9, Kernel: rbf, Validation Accuracy: 0.8623188405797102\n",
      "C: 0.9, Kernel: sigmoid, Validation Accuracy: 0.8985507246376812\n",
      "C: 1.0, Kernel: linear, Validation Accuracy: 0.8840579710144928\n",
      "C: 1.0, Kernel: poly, Validation Accuracy: 0.5942028985507246\n",
      "C: 1.0, Kernel: rbf, Validation Accuracy: 0.855072463768116\n",
      "C: 1.0, Kernel: sigmoid, Validation Accuracy: 0.8913043478260869\n",
      "C: 1.1, Kernel: linear, Validation Accuracy: 0.8840579710144928\n",
      "C: 1.1, Kernel: poly, Validation Accuracy: 0.6666666666666666\n",
      "C: 1.1, Kernel: rbf, Validation Accuracy: 0.855072463768116\n",
      "C: 1.1, Kernel: sigmoid, Validation Accuracy: 0.8840579710144928\n",
      "C: 1.25, Kernel: linear, Validation Accuracy: 0.8840579710144928\n",
      "C: 1.25, Kernel: poly, Validation Accuracy: 0.6739130434782609\n",
      "C: 1.25, Kernel: rbf, Validation Accuracy: 0.8623188405797102\n",
      "C: 1.25, Kernel: sigmoid, Validation Accuracy: 0.8768115942028986\n",
      "C: 1.5, Kernel: linear, Validation Accuracy: 0.8695652173913043\n",
      "C: 1.5, Kernel: poly, Validation Accuracy: 0.6739130434782609\n",
      "C: 1.5, Kernel: rbf, Validation Accuracy: 0.8695652173913043\n",
      "C: 1.5, Kernel: sigmoid, Validation Accuracy: 0.8623188405797102\n",
      "C: 2.0, Kernel: linear, Validation Accuracy: 0.855072463768116\n",
      "C: 2.0, Kernel: poly, Validation Accuracy: 0.6739130434782609\n",
      "C: 2.0, Kernel: rbf, Validation Accuracy: 0.8695652173913043\n",
      "C: 2.0, Kernel: sigmoid, Validation Accuracy: 0.8478260869565217\n",
      "C: 5.0, Kernel: linear, Validation Accuracy: 0.8188405797101449\n",
      "C: 5.0, Kernel: poly, Validation Accuracy: 0.6739130434782609\n",
      "C: 5.0, Kernel: rbf, Validation Accuracy: 0.8695652173913043\n",
      "C: 5.0, Kernel: sigmoid, Validation Accuracy: 0.7971014492753623\n",
      "C: 10.0, Kernel: linear, Validation Accuracy: 0.8260869565217391\n",
      "C: 10.0, Kernel: poly, Validation Accuracy: 0.6739130434782609\n",
      "C: 10.0, Kernel: rbf, Validation Accuracy: 0.8695652173913043\n",
      "C: 10.0, Kernel: sigmoid, Validation Accuracy: 0.8043478260869565\n",
      "Best C: 0.9, Kernel: sigmoid with Validation Accuracy: 0.8985507246376812\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "Test Accuracy: 0.8561151079136691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87        78\n",
      "           1       0.82      0.87      0.84        61\n",
      "\n",
      "    accuracy                           0.86       139\n",
      "   macro avg       0.85      0.86      0.85       139\n",
      "weighted avg       0.86      0.86      0.86       139\n",
      "\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Test Accuracy: 0.8705035971223022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        78\n",
      "           1       0.85      0.85      0.85        61\n",
      "\n",
      "    accuracy                           0.87       139\n",
      "   macro avg       0.87      0.87      0.87       139\n",
      "weighted avg       0.87      0.87      0.87       139\n",
      "\n",
      "\n",
      "Evaluating SVM...\n",
      "Test Accuracy: 0.8776978417266187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89        78\n",
      "           1       0.84      0.89      0.86        61\n",
      "\n",
      "    accuracy                           0.88       139\n",
      "   macro avg       0.88      0.88      0.88       139\n",
      "weighted avg       0.88      0.88      0.88       139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "data, labels = load_data(facts_file='facts_more.txt', fakes_file='fakes_more.txt')\n",
    "#data_preprocessed = data\n",
    "data_preprocessed = [preprocess_data(line) for line in data]\n",
    "X, vectorizer = feature_extract_data(data_preprocessed)\n",
    "\n",
    "# Step 2: Split data into training, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data_70_15_15(X, labels)\n",
    "\n",
    "# Step 3a: Train Naive Bayes with hyperparameter tuning\n",
    "print(\"\\nTraining Naive Bayes...\")\n",
    "best_nb_model, best_alpha = train_naive_bayes(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Step 3b: Train Logistic Regression with hyperparameter tuning\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "best_lr_model, lr_best_C, best_penalty = train_logistic_regression(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Step 3c: Train SVM with hyperparameter tuning\n",
    "print(\"\\nTraining SVM...\")\n",
    "best_svm_model, svm_best_C, best_kernel = train_svm(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Step 4: Evaluate all models on the test set\n",
    "print(\"\\nEvaluating Naive Bayes...\")\n",
    "nb_acc = evaluate_model(best_nb_model, X_test, y_test)\n",
    "\n",
    "print(\"\\nEvaluating Logistic Regression...\")\n",
    "lr_acc = evaluate_model(best_lr_model, X_test, y_test)\n",
    "\n",
    "print(\"\\nEvaluating SVM...\")\n",
    "svm_acc = evaluate_model(best_svm_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
